{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwfOJRd_mvaE",
        "outputId": "45b4e1a6-a2d4-4e53-f3db-6af2834d4295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566164 sha256=859a8a2e6a9459514029443d697b49d408781711891272baf8e3998faf5b3927\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41_0WSYJL6cZ",
        "outputId": "1cff531e-3b29-46f6-880e-83c2ac6dea87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjn-2Yao_br8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLKHVlM5lZlo",
        "outputId": "bd38c382-b7d5-45b0-ad8a-0946261ab1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHQ3pD9qlgu6",
        "outputId": "385435a3-db95-4db4-8320-0cd1ef22af74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-bootstrap\n",
            "  Downloading Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m450.6/456.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-bootstrap) (2.2.5)\n",
            "Collecting dominate (from flask-bootstrap)\n",
            "  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting visitor (from flask-bootstrap)\n",
            "  Downloading visitor-0.1.3.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-bootstrap) (2.1.5)\n",
            "Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: flask-bootstrap, visitor\n",
            "  Building wheel for flask-bootstrap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flask-bootstrap: filename=Flask_Bootstrap-3.3.7.1-py3-none-any.whl size=460119 sha256=54ba90d36446b15e1817e05547b4c2c1f950d330666760aa1a3d7f5fcc03a51a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/33/ad/26540e84a28334e5dfeda756df270f95353779f03bc5cf40d4\n",
            "  Building wheel for visitor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visitor: filename=visitor-0.1.3-py3-none-any.whl size=3926 sha256=e1f9ad656e0145832eb7389cf65de4ac756bb572625c16733141e860c6ed036f\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/31/99/2ec5b4459cac4d801d6201d501a354366d180afc9f8bb2d333\n",
            "Successfully built flask-bootstrap visitor\n",
            "Installing collected packages: visitor, dominate, flask-bootstrap\n",
            "Successfully installed dominate-2.9.1 flask-bootstrap-3.3.7.1 visitor-0.1.3\n"
          ]
        }
      ],
      "source": [
        "pip install flask-bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWSA6fZ2fkeT"
      },
      "outputs": [],
      "source": [
        "port_no = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAWAAg9UgSYa"
      },
      "outputs": [],
      "source": [
        "import face_recognition as fr\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "class FaceRecognition:\n",
        "    def __init__(self, frames_dir, known_face_encodings, student_dic):\n",
        "        self.frames_dir = frames_dir\n",
        "        self.known_face_encodings = known_face_encodings\n",
        "        self.students = student_dic\n",
        "        self.sim = []\n",
        "        csm_students = list(self.students.keys())\n",
        "\n",
        "    def load_image_paths(self):\n",
        "        image_paths = [os.path.join(self.frames_dir, filename) for filename in os.listdir(self.frames_dir) if filename.lower().endswith('.jpg')]\n",
        "        return image_paths\n",
        "\n",
        "    def load_new_student(path):\n",
        "        frame = fr.load_image_file(path)\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "        face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='cnn')\n",
        "        x_face_encoding = fr.face_encodings(frame, face_locations)\n",
        "        return x_face_encoding\n",
        "\n",
        "    def find_faces(self, image_paths):\n",
        "        face_locations = []\n",
        "        face_encodings = []\n",
        "\n",
        "        for path in image_paths:\n",
        "            frame = fr.load_image_file(path)\n",
        "            rgb_frame = frame[:, :, ::-1]  # Convert BGR to RGB\n",
        "\n",
        "            face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='cnn')\n",
        "            face_encodings_for_frame = fr.face_encodings(frame, face_locations)\n",
        "\n",
        "            face_encodings.extend(face_encodings_for_frame)\n",
        "\n",
        "        return face_encodings\n",
        "\n",
        "\n",
        "\n",
        "    def cosine_similarity(self, vector_a, vector_b):\n",
        "        vector_a = np.array(vector_a, dtype=float)\n",
        "        vector_b = np.array(vector_b, dtype=float)\n",
        "        dot_product = np.dot(vector_a, vector_b)\n",
        "        magnitude_a = np.linalg.norm(vector_a)\n",
        "        magnitude_b = np.linalg.norm(vector_b)\n",
        "        if magnitude_a == 0 or magnitude_b == 0:\n",
        "            return 0\n",
        "\n",
        "        cosine_similarity_value = dot_product / (magnitude_a * magnitude_b)\n",
        "        return cosine_similarity_value\n",
        "\n",
        "\n",
        "\n",
        "    def compute_similarities(self,video_face_encodings, known_face_encodings,csm_students):\n",
        "        similarities = []\n",
        "        count = 0\n",
        "        for y in video_face_encodings:\n",
        "            nec = [self.cosine_similarity(y, x) for x in known_face_encodings]\n",
        "            similarities.append(nec)\n",
        "            count += 1\n",
        "        return similarities, count\n",
        "\n",
        "    def compare_faces(self, video_face_encodings):\n",
        "        similarities = []\n",
        "        student_names = []\n",
        "        csm_students = list(self.students.keys())\n",
        "        count = 0\n",
        "        similarities, count = self.compute_similarities(video_face_encodings, self.known_face_encodings,csm_students)\n",
        "\n",
        "\n",
        "        for x in similarities:\n",
        "            student_names.append(csm_students[np.argmax(x)])\n",
        "        print(len(video_face_encodings))\n",
        "        return list(set(student_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMiobEda1i4B"
      },
      "outputs": [],
      "source": [
        "import face_recognition as fr\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "class FaceRecognition:\n",
        "    def __init__(self, frames_dir, known_face_encodings, student_dic):\n",
        "        self.frames_dir = frames_dir\n",
        "        self.known_face_encodings = known_face_encodings\n",
        "        self.students = student_dic\n",
        "        self.sim = []\n",
        "        csm_students = list(self.students.keys())\n",
        "\n",
        "    def load_image_paths(self):\n",
        "        image_paths = [os.path.join(self.frames_dir, filename) for filename in os.listdir(self.frames_dir) if filename.lower().endswith('.jpg')]\n",
        "        return image_paths\n",
        "\n",
        "    def load_new_student(self,path):\n",
        "        frame = fr.load_image_file(path)\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "        face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='cnn')\n",
        "        x_face_encoding = fr.face_encodings(frame, face_locations)\n",
        "        return x_face_encoding\n",
        "\n",
        "    def find_faces(self, image_paths):\n",
        "        face_locations = []\n",
        "        face_encodings = []\n",
        "\n",
        "        for path in image_paths:\n",
        "            frame = fr.load_image_file(path)\n",
        "            rgb_frame = frame[:, :, ::-1]  # Convert BGR to RGB\n",
        "\n",
        "            face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='cnn')\n",
        "            face_encodings_for_frame = fr.face_encodings(frame, face_locations)\n",
        "\n",
        "            face_encodings.extend(face_encodings_for_frame)\n",
        "\n",
        "        return face_encodings\n",
        "\n",
        "\n",
        "\n",
        "    def cosine_similarity(self, vector_a, vector_b):\n",
        "        vector_a = np.array(vector_a, dtype=float)\n",
        "        vector_b = np.array(vector_b, dtype=float)\n",
        "        dot_product = np.dot(vector_a, vector_b)\n",
        "        magnitude_a = np.linalg.norm(vector_a)\n",
        "        magnitude_b = np.linalg.norm(vector_b)\n",
        "        if magnitude_a == 0 or magnitude_b == 0:\n",
        "            return 0\n",
        "\n",
        "        cosine_similarity_value = dot_product / (magnitude_a * magnitude_b)\n",
        "        return cosine_similarity_value\n",
        "\n",
        "\n",
        "\n",
        "    def compute_similarities(self,video_face_encodings, known_face_encodings,csm_students):\n",
        "        similarities = []\n",
        "        count = 0\n",
        "        for y in video_face_encodings:\n",
        "            nec = [self.cosine_similarity(y, x) for x in known_face_encodings]\n",
        "            similarities.append(nec)\n",
        "            count += 1\n",
        "        return similarities, count\n",
        "\n",
        "    def compare_faces(self, video_face_encodings):\n",
        "        similarities = []\n",
        "        student_names = []\n",
        "        csm_students = list(self.students.keys())\n",
        "        count = 0\n",
        "        similarities, count = self.compute_similarities(video_face_encodings, self.known_face_encodings,csm_students)\n",
        "\n",
        "\n",
        "        for x in similarities:\n",
        "            student_names.append(csm_students[np.argmax(x)])\n",
        "        print(len(video_face_encodings))\n",
        "        return list(set(student_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfeFE6yy1lvL"
      },
      "outputs": [],
      "source": [
        "import face_recognition as fr\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "class FaceRecognition:\n",
        "    def __init__(self, frames_dir, known_face_encodings, student_dic):\n",
        "        self.frames_dir = frames_dir\n",
        "        self.known_face_encodings = known_face_encodings\n",
        "        self.students = student_dic\n",
        "        self.sim = []\n",
        "        csm_students = list(self.students.keys())\n",
        "\n",
        "    def load_image_paths(self):\n",
        "        image_paths = [os.path.join(self.frames_dir, filename) for filename in os.listdir(self.frames_dir) if filename.lower().endswith('.jpg')]\n",
        "        return image_paths\n",
        "\n",
        "    def load_new_student(path):\n",
        "        frame = fr.load_image_file(path)\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "        face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='cnn')\n",
        "        x_face_encoding = fr.face_encodings(frame, face_locations)\n",
        "        return x_face_encoding\n",
        "\n",
        "    def find_faces( image_paths):\n",
        "        face_locations = []\n",
        "        face_encodings = []\n",
        "\n",
        "        for path in image_paths:\n",
        "            frame = fr.load_image_file(path)\n",
        "            rgb_frame = frame[:, :, ::-1]  # Convert BGR to RGB\n",
        "\n",
        "            face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='cnn')\n",
        "            face_encodings_for_frame = fr.face_encodings(frame, face_locations)\n",
        "\n",
        "            face_encodings.extend(face_encodings_for_frame)\n",
        "\n",
        "        return face_encodings\n",
        "\n",
        "\n",
        "\n",
        "    def cosine_similarity( vector_a, vector_b):\n",
        "        vector_a = np.array(vector_a, dtype=float)\n",
        "        vector_b = np.array(vector_b, dtype=float)\n",
        "        dot_product = np.dot(vector_a, vector_b)\n",
        "        magnitude_a = np.linalg.norm(vector_a)\n",
        "        magnitude_b = np.linalg.norm(vector_b)\n",
        "        if magnitude_a == 0 or magnitude_b == 0:\n",
        "            return 0\n",
        "\n",
        "        cosine_similarity_value = dot_product / (magnitude_a * magnitude_b)\n",
        "        return cosine_similarity_value\n",
        "\n",
        "\n",
        "\n",
        "    def compute_similarities(video_face_encodings, known_face_encodings,csm_students):\n",
        "        similarities = []\n",
        "        count = 0\n",
        "        for y in video_face_encodings:\n",
        "            nec = [FaceRecognition.cosine_similarity(y, x) for x in known_face_encodings]\n",
        "            similarities.append(nec)\n",
        "            count += 1\n",
        "        return similarities, count\n",
        "\n",
        "    def compare_faces(self, video_face_encodings):\n",
        "        similarities = []\n",
        "        student_names = []\n",
        "        csm_students = list(self.students.keys())\n",
        "        count = 0\n",
        "        similarities, count = self.compute_similarities(video_face_encodings, self.known_face_encodings,csm_students)\n",
        "\n",
        "\n",
        "        for x in similarities:\n",
        "            student_names.append(csm_students[np.argmax(x)])\n",
        "        print(len(video_face_encodings))\n",
        "        return list(set(student_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B31-oUeH_pD8"
      },
      "outputs": [],
      "source": [
        "@jit(nopython=True, parallel=True)\n",
        "def compute_similarities(video_face_encodings, known_face_encodings):\n",
        "        similarities = []\n",
        "        count = 0\n",
        "        obj = FaceRecognition()\n",
        "        for y in video_face_encodings:\n",
        "            for x in known_face_encodings:\n",
        "                nec = obj.cosine_similarity(y,x)\n",
        "                similarities.append(csm_students(np.argmax(nec)))\n",
        "            known_face_encodings.pop(np.argmax(nec))\n",
        "            count += 1\n",
        "        return similarities, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "AuPTX3EegYOF",
        "outputId": "2ad4fc42-186f-418c-b69a-5fda1a666b2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'if __name__ == \"__main__\":\\n    directory = \"Frames\"\\n    video_path = \"video.mp4\"\\n\\n    if not os.path.exists(directory):\\n        os.makedirs(directory)\\n\\n    preprocessor = VideoPreprocessor(directory, video_path)\\n    preprocessor.extract_frames_per_second()'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import face_recognition as fr\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "\n",
        "class VideoPreprocessor:\n",
        "    def __init__(self, directory, video_path):\n",
        "        self.directory = directory\n",
        "        self.video_path = video_path\n",
        "        self.fps = None\n",
        "        self.frame_count = 0\n",
        "        self.extracted_frame_count = 0\n",
        "\n",
        "    def empty_folder(self):\n",
        "        for file in os.listdir(self.directory):\n",
        "            file_path = os.path.join(self.directory, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        return True\n",
        "\n",
        "    def extract_frames_per_second(self):\n",
        "        try:\n",
        "            cap = cv.VideoCapture(self.video_path)\n",
        "            if not cap.isOpened():\n",
        "                raise IOError(\"Error opening video!\")\n",
        "\n",
        "            self.fps = cap.get(cv.CAP_PROP_FPS)\n",
        "            if self.fps <= 0:\n",
        "                raise ValueError(\"Invalid FPS value.\")\n",
        "\n",
        "            if self.empty_folder():\n",
        "                while True:\n",
        "                    ret, frame = cap.read()\n",
        "\n",
        "                    if not ret:\n",
        "                        print(\"End of video reached.\")\n",
        "                        break\n",
        "\n",
        "                    # Extract one frame per second\n",
        "                    if self.frame_count % int(self.fps) == 0:\n",
        "                        filename = f\"{self.directory}/frame_{self.extracted_frame_count}.jpg\"\n",
        "                        cv.imwrite(filename, frame)\n",
        "                        self.extracted_frame_count += 1\n",
        "\n",
        "                    self.frame_count += 1\n",
        "\n",
        "                print(f\"Extracted {self.extracted_frame_count} frames to {self.directory}\")\n",
        "\n",
        "        except IOError as e:\n",
        "            print(f\"IOError: {e}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"ValueError: {e}\")\n",
        "        finally:\n",
        "            cap.release()\n",
        "\n",
        "# Example usage\n",
        "'''if __name__ == \"__main__\":\n",
        "    directory = \"Frames\"\n",
        "    video_path = \"video.mp4\"\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    preprocessor = VideoPreprocessor(directory, video_path)\n",
        "    preprocessor.extract_frames_per_second()'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V_E2qXY8Lyh",
        "outputId": "a3bcf8ea-c84f-4a89-a2e2-f34e980cf6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'21981a4465': 'Sudha Sankar rao', '21981a4466': 'Harsha Vardhan'} [array([-0.13409157,  0.13800862,  0.05355558, -0.05370569, -0.06824048,\n",
            "       -0.04218274, -0.03351473, -0.10006957,  0.17636909, -0.09130377,\n",
            "        0.23674241, -0.08588597, -0.20948203, -0.12220045,  0.02624761,\n",
            "        0.09816415, -0.13268098, -0.1231949 , -0.02000012, -0.14298905,\n",
            "        0.02302869,  0.01699419,  0.02203441,  0.02601515, -0.19595644,\n",
            "       -0.3748391 , -0.09661537, -0.13424091, -0.02401798, -0.05964912,\n",
            "       -0.01487449,  0.06595365, -0.1990841 , -0.01786499,  0.00695241,\n",
            "        0.15933605, -0.02809027,  0.0147193 ,  0.15072964, -0.00298209,\n",
            "       -0.15610576, -0.05875939,  0.05332009,  0.22004597,  0.1062866 ,\n",
            "        0.09089354,  0.03621432, -0.01376179,  0.07284073, -0.11803743,\n",
            "        0.10290713,  0.17175156,  0.09627068,  0.03114554,  0.07370009,\n",
            "       -0.13480112, -0.00356767,  0.02902903, -0.1969893 , -0.02877663,\n",
            "        0.07093938, -0.06295201, -0.11513643,  0.0131266 ,  0.25525409,\n",
            "        0.15786698, -0.08176979, -0.12105227,  0.18613163, -0.15698513,\n",
            "       -0.01741213,  0.11648855, -0.09973602, -0.18072733, -0.18099687,\n",
            "        0.11777619,  0.41321412,  0.07240836, -0.12010649,  0.02142368,\n",
            "       -0.02828327, -0.08779524,  0.06264971,  0.08057605, -0.10684548,\n",
            "        0.10201148, -0.09868585,  0.10542156,  0.17566845, -0.0384904 ,\n",
            "       -0.02009021,  0.17669271, -0.01935061,  0.04595475,  0.03232023,\n",
            "        0.00336507, -0.07818182,  0.03930726, -0.06453767, -0.03124446,\n",
            "        0.17771938, -0.08328466, -0.06642504,  0.03235259, -0.18346131,\n",
            "        0.12912115, -0.0012145 , -0.07982046, -0.07416697, -0.0216701 ,\n",
            "       -0.06855237, -0.03494355,  0.15650028, -0.26451695,  0.17535141,\n",
            "        0.12025398, -0.04282489,  0.20084912,  0.0512448 ,  0.06830701,\n",
            "        0.05611813, -0.09743866, -0.14430425, -0.10735733,  0.06915177,\n",
            "       -0.03063141,  0.08807868,  0.00318425]), array([-0.14687368,  0.01456255,  0.00958664, -0.0906373 , -0.0347506 ,\n",
            "       -0.02814522, -0.04816662, -0.07201219,  0.18186997, -0.05262711,\n",
            "        0.18903413, -0.01573794, -0.10202901, -0.15238808, -0.04394409,\n",
            "        0.08903156, -0.22752291, -0.15912607,  0.07723728, -0.04753591,\n",
            "        0.03216673, -0.01582479,  0.0131561 ,  0.09124845, -0.22455089,\n",
            "       -0.27669069, -0.10768033, -0.10294095,  0.05011374, -0.02970619,\n",
            "       -0.01691376,  0.08687535, -0.28536659, -0.05618397,  0.04192267,\n",
            "        0.22503185,  0.04101982, -0.02289622,  0.13595706,  0.0723049 ,\n",
            "       -0.12464093, -0.08220944, -0.00466462,  0.3078019 ,  0.16916782,\n",
            "        0.01912043, -0.03434774, -0.03070135,  0.07104126, -0.23741747,\n",
            "        0.1053315 ,  0.13758168,  0.1232184 , -0.00968777,  0.00666855,\n",
            "       -0.1276011 ,  0.05288792,  0.11273243, -0.19587691,  0.0067393 ,\n",
            "       -0.00634798, -0.04833999, -0.02201466, -0.00698339,  0.22678618,\n",
            "        0.15594845, -0.09091264, -0.14867142,  0.22299436, -0.11133648,\n",
            "       -0.03823413,  0.07677037, -0.06632991, -0.15095434, -0.35024399,\n",
            "        0.03913257,  0.42800948,  0.11596572, -0.19073668,  0.04809583,\n",
            "       -0.12053512, -0.00348842,  0.08458771,  0.07046823, -0.1053131 ,\n",
            "        0.07375275, -0.13086224,  0.03368329,  0.16911384,  0.03291678,\n",
            "       -0.06798425,  0.12386519, -0.01227004,  0.03331785,  0.08444626,\n",
            "        0.01747566, -0.03388648,  0.03993034, -0.15609565, -0.03680126,\n",
            "        0.06560834, -0.0336534 , -0.01261547,  0.2136699 , -0.21687877,\n",
            "        0.08618917,  0.05658762, -0.13602597, -0.00094267,  0.00112754,\n",
            "       -0.12152977, -0.07914868,  0.12221392, -0.32238334,  0.13484418,\n",
            "        0.16859089,  0.01711661,  0.19920158,  0.08026381,  0.06231686,\n",
            "       -0.00676179, -0.06984653, -0.12635076, -0.03838142,  0.01344798,\n",
            "       -0.03877921,  0.05556858,  0.02998889])]\n"
          ]
        }
      ],
      "source": [
        "attendees_list=[\"21981a4465\"]\n",
        "with sqlite3.connect(\"Attendance.db\") as conn:\n",
        "    with sqlite3.connect(\"Attendance.db\") as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT * FROM USERS\")\n",
        "        student_dict = {}\n",
        "        new_Face_Encodings = []\n",
        "        for i in cursor :\n",
        "            student_dict[i[1]] = i[0]\n",
        "            new_Face_Encodings.append(np.array([float(j) for j in i[2].split(\"/\") ]))\n",
        "        print(student_dict,new_Face_Encodings)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SZTJrxn3hUb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgR3KSiSZ6gW",
        "outputId": "5412b6d2-f35b-4085-fdfc-733b46aa5b84"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://616a-34-125-174-152.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Aug/2024 02:07:34] \"GET / HTTP/1.1\" 200 -\n",
            "ERROR:__main__:Exception on /findFace [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-13-8312081b32d5>\", line 132, in findFace\n",
            "    face_encoded_data = FaceRecognition.find_faces(image.filename)\n",
            "  File \"<ipython-input-9-aaf4988126a8>\", line 31, in find_faces\n",
            "    frame = fr.load_image_file(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/face_recognition/api.py\", line 86, in load_image_file\n",
            "    im = PIL.Image.open(file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3227, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'C'\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Aug/2024 02:08:10] \"\u001b[35m\u001b[1mPOST /findFace HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAP7244570888850627228.jpg\n"
          ]
        }
      ],
      "source": [
        "from flask import *\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2fhOSoa9jSR3KoJHNOmgUultPE2_6jkfdizMSEioQtYY1Q35s\")\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "new_List = []\n",
        "app = Flask(__name__)\n",
        "app.secret_key='I_AM_IRON_MAN'\n",
        "ALLOWED_EXTENSIONS = ['mp4']\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return render_template(\"newForm.html\")\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "@app.route(\"/uploadStudents\",methods=[\"POST\",\"GET\"])\n",
        "def ImageUpload():\n",
        "    if request.method == 'POST':\n",
        "        name = request.form[\"Name\"]\n",
        "        roll = request.form[\"Roll\"]\n",
        "        image = request.files['Image']\n",
        "        image.save(image.filename)\n",
        "        print(image)\n",
        "        face_encoded = FaceRecognition.load_new_student(image)\n",
        "        new_List.append(face_encoded)\n",
        "        new_array = \"/\".join([str(i) for i in face_encoded[0]])\n",
        "        print(new_array)\n",
        "        if SelectFromTable(roll):\n",
        "            return jsonify(\"Already present in the database\")\n",
        "        with sqlite3.connect(\"raghu_engg_attendance.db\") as conn :\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"INSERT INTO RIT_CSM (name , roll , face_encodings ) VALUES (?,?,?)\",(name,roll,new_array))\n",
        "            conn.commit()\n",
        "        return jsonify(\"SuccessFully Uploaded\")\n",
        "@app.route(\"/uploadVideo\")\n",
        "def videoUpload():\n",
        "  return render_template(\"index.html\")\n",
        "@app.route(\"/uploadDetailedVideo\",methods = [\"POST\",\"GET\"])\n",
        "def detailedVideoUpload():\n",
        "    if 'video' not in request.files:\n",
        "        return  jsonify(\"No Video Found\")\n",
        "    video = request.files['video']\n",
        "    if video.filename == \"\":\n",
        "        return jsonify(\"No File Selected\")\n",
        "    if video and allowed_file(video.filename):\n",
        "        video.save(video.filename)\n",
        "        video_object = VideoPreprocessor(\"/content/Frames\",video_path=video.filename)\n",
        "        if video_object.empty_folder():\n",
        "            video_object.extract_frames_per_second()\n",
        "            studentdetails,known_encodings = fetchFromDataBase()\n",
        "            face_object = FaceRecognition(\"/content/Frames\",known_encodings,studentdetails)\n",
        "            new_image_paths = face_object.load_image_paths()\n",
        "            new_face_encodings = face_object.find_faces(new_image_paths)\n",
        "            attendees_list = face_object.compare_faces(new_face_encodings)\n",
        "            new_dictionary = joining_sequel_function(attendees_list)\n",
        "            return jsonify(new_dictionary)\n",
        "        return jsonify(\"Error in emptying the Folder\")\n",
        "    return jsonify(\"Invalid type in the File \")\n",
        "\n",
        "\"\"\"def joining_sequel_function(attendees_list):\n",
        "  with sqlite3.connect(\"raghu_engg_attendance.db\") as conn:\n",
        "    print(attendees_list)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT * FROM RIT_CSM WHERE roll IN (%s)\" % ','.join('?' for _ in attendees_list), attendees_list)\n",
        "    attendees_dict = { i[1] : i[0] for i in cursor }\n",
        "    print(attendees_dict)\n",
        "    dictionary = {}\n",
        "    array_presents = []\n",
        "    for i in attendees_list:\n",
        "      array_presents.append({\"name\":attendees_dict[i],\"roll\":i})\n",
        "    dictionary[\"presentees\"] = array_presents\n",
        "    return dictionary\"\"\"\n",
        "def joining_sequel_function(attendees_list):\n",
        "  with sqlite3.connect(\"raghu_engg_attendance.db\") as conn:\n",
        "    print(attendees_list)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT * FROM RIT_CSM\")\n",
        "    all_attendees_list = { i[1] : i[0] for i in cursor }\n",
        "    print(all_attendees_list)\n",
        "    abseentees_list = [ i for i in all_attendees_list.keys() if i not in attendees_list ]\n",
        "    print(abseentees_list)\n",
        "    dictionary = {}\n",
        "    array_presents = []\n",
        "    array_absents  = []\n",
        "    for i in attendees_list:\n",
        "      array_presents.append({\"name\":all_attendees_list[i],\"roll\":i})\n",
        "    dictionary[\"presentees\"] = array_presents\n",
        "    for i in abseentees_list:\n",
        "      array_absents.append({\"name\":all_attendees_list[i],\"roll\":i})\n",
        "    dictionary[\"absentees\"] = array_absents\n",
        "    print(dictionary)\n",
        "    return dictionary\n",
        "def fetchFromDataBase():\n",
        "    with sqlite3.connect(\"raghu_engg_attendance.db\") as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT * FROM RIT_CSM\")\n",
        "        student_dict = {}\n",
        "        new_Face_Encodings = []\n",
        "        for i in cursor :\n",
        "            student_dict[i[1]] = i[0]\n",
        "            new_Face_Encodings.append(np.array([float(j) for j in i[2].split(\"/\") ]))\n",
        "    return student_dict,new_Face_Encodings\n",
        "@app.route(\"/generatePDF\",methods = [\"POST\",\"GET\"])\n",
        "def lastTouch():\n",
        "  if request.method == \"POST\":\n",
        "      get_text = request.form[\"textdata\"]\n",
        "      new_text_speech = generator(get_text)\n",
        "      return jsonify(new_text_speech)\n",
        "def createTable():\n",
        "       with sqlite3.connect(\"raghu_engg_attendance.db\") as conn:\n",
        "         cursor = conn.cursor()\n",
        "         cursor.execute(\"CREATE TABLE RIT_CSM (name VARCHAR(50) , roll VARCHAR(50) , face_encodings TEXT(16000), PRIMARY KEY(roll))\")\n",
        "       print(\"Created successfully\")\n",
        "def SelectFromTable(roll_no):\n",
        "    with sqlite3.connect(\"raghu_engg_attendance.db\") as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT * FROM RIT_CSM ORDER BY roll\")\n",
        "        another_tuple = []\n",
        "        for i in cursor:\n",
        "            another_tuple.append(i[1])\n",
        "        if roll_no in another_tuple:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "@app.route(\"/findFace\", methods = [\"POST\", \"GET\"])\n",
        "def findFace():\n",
        "    if request.method==\"POST\":\n",
        "      image =  request.files[\"Details\"]\n",
        "      image.save(image.filename)\n",
        "      face_encoded_data = FaceRecognition.find_faces(image.filename)\n",
        "      print(face_encoded_data)\n",
        "      another_dictionary = {}\n",
        "      with sqlite3.connect(\"raghu_engg_attendance.db\") as conn:\n",
        "          cursor = conn.cursor()\n",
        "          cursor.execute(\"SELECT * FROM RIT_CSM\")\n",
        "          list_of_numpy = []\n",
        "          for i in cursor:\n",
        "               list_of_numpy.append(np.array([float(j) for j in i[2].split(\"/\") ]))\n",
        "          similarities_of_new = []\n",
        "          for x in list_of_numpy:\n",
        "               simi = FaceRecognition.cosine_similarity(face_encoded_data,x)\n",
        "               similarities_of_new.append(simi)\n",
        "          index = np.argmax(similarities_of_new)\n",
        "          row = cursor[index]\n",
        "          another_dictionary[\"Roll\"] = row[1]\n",
        "          another_dictionary[\"Name\"] = row[0]\n",
        "          another_dictionary[\"Section\"] = \"CSM\"\n",
        "          print(another_dictionary)\n",
        "      return jsonify(another_dictionary)\n",
        "if __name__ == '__main__':\n",
        "    createTable()\n",
        "    print(public_url)\n",
        "    app.run(port = port_no )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAiRCOE9sp1L",
        "outputId": "ec09460f-3cfd-4250-dcd8-ff3685fa0fc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ True,  True,  True])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([1,2,3])==np.array([1,2,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUm_1fxU7t_u"
      },
      "outputs": [],
      "source": [
        "pip -q install crewai langchain_groq langchain_community duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XYsR6-2jAN1",
        "outputId": "4fd4a599-629e-468f-9697-fe78d2a4f331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.5-py3-none-any.whl (11 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3,>=0.1.45 (from langchain_groq)\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.45->langchain_groq)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.66 (from langchain-core<0.3,>=0.1.45->langchain_groq)\n",
            "  Downloading langsmith-0.1.75-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<24.0,>=23.2 (from langchain-core<0.3,>=0.1.45->langchain_groq)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (8.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.45->langchain_groq)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.1.45->langchain_groq)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.1.45->langchain_groq) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.1.45->langchain_groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.1.45->langchain_groq) (2.0.7)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, h11, jsonpatch, httpcore, langsmith, httpx, langchain-core, groq, langchain_groq\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed groq-0.8.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-core-0.2.5 langchain_groq-0.1.5 langsmith-0.1.75 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSyNRqWRjGEK",
        "outputId": "c81ebd9c-9702-48f9-9eb5-e2ccef1e9276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Using cached duckduckgo_search-6.1.5-py3-none-any.whl (24 kB)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.30.11-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Collecting pyreqwest-impersonate>=0.4.7 (from duckduckgo-search)\n",
            "  Downloading pyreqwest_impersonate-0.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.3 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (3.10.3)\n",
            "Collecting appdirs<2.0.0,>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai)\n",
            "  Downloading embedchain-0.1.108-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.2/195.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting instructor<0.6.0,>=0.5.2 (from crewai)\n",
            "  Downloading instructor-0.5.2-py3-none-any.whl (33 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.10 (from crewai)\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.13.3 (from crewai)\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.7.3)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from crewai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting regex<2024.0.0,>=2023.12.25 (from crewai)\n",
            "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic<2.0.0,>=1.13.1 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (4.12.3)\n",
            "Collecting chromadb<0.6.0,>=0.5.0 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (1.52.0)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading gptcache-0.1.43-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading langchain_cohere-0.1.5-py3-none-any.whl (30 kB)\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
            "Collecting posthog<4.0.0,>=3.0.2 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (13.7.1)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (2.0.30)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (3.9.5)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from instructor<0.6.0,>=0.5.2->crewai)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (8.3.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (0.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (6.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (0.1.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (2.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (4.12.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai) (7.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (1.63.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.25.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (3.20.3)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (2.18.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.9.4)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (1.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain<0.2.0,>=0.1.98->crewai) (2.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.2.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes>=28.1.0 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai) (1.14.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (3.21.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.0.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain<0.2.0,>=0.1.98->crewai) (5.3.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai) (3.19.1)\n",
            "Collecting cohere<6.0,>=5.5 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading cohere-5.5.6-py3-none-any.whl (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.10->crewai) (1.33)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.10->crewai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.10->crewai) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain<0.2.0,>=0.1.98->crewai) (3.0.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (2.0.1)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading boto3-1.34.122-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx-sse<0.5.0,>=0.4.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting tokenizers>=0.13.2 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl (15 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.7.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.10->crewai) (2.4)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (3.2.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (0.1.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.12.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
            "Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (0.23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai) (2.1.5)\n",
            "Collecting botocore<1.35.0,>=1.34.122 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading botocore-1.34.122-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of fastapi-cli to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "  Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n",
            "Collecting fastapi>=0.95.2 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (2023.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.3.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=ff143c7923ad5f9a3e7937f76027500b95216f558bfa3edbb7517506fa15a4d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: schema, pypika, monotonic, mmh3, appdirs, websockets, uvloop, uvicorn, types-requests, regex, python-dotenv, pysbd, pyreqwest-impersonate, pypdf, parameterized, overrides, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, marshmallow, Mako, jmespath, humanfriendly, httpx-sse, httptools, fastavro, docstring-parser, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, gptcache, duckduckgo-search, coloredlogs, botocore, alembic, tokenizers, s3transfer, opentelemetry-semantic-conventions, opentelemetry-instrumentation, openai, onnxruntime, kubernetes, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langchain-core, instructor, boto3, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain-community, cohere, langchain-cohere, langchain, chromadb, embedchain, crewai\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.5.15\n",
            "    Uninstalling regex-2024.5.15:\n",
            "      Successfully uninstalled regex-2024.5.15\n",
            "  Attempting uninstall: docstring-parser\n",
            "    Found existing installation: docstring_parser 0.16\n",
            "    Uninstalling docstring_parser-0.16:\n",
            "      Successfully uninstalled docstring_parser-0.16\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.5\n",
            "    Uninstalling langchain-core-0.2.5:\n",
            "      Successfully uninstalled langchain-core-0.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.41.2 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 alembic-1.13.1 appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 boto3-1.34.122 botocore-1.34.122 chroma-hnswlib-0.7.3 chromadb-0.5.0 cohere-5.5.6 coloredlogs-15.0.1 crewai-0.30.11 dataclasses-json-0.6.6 deprecated-1.2.14 docstring-parser-0.15 duckduckgo-search-6.1.5 embedchain-0.1.108 fastapi-0.110.3 fastavro-1.9.4 gptcache-0.1.43 httptools-0.6.1 httpx-sse-0.4.0 humanfriendly-10.0 instructor-0.5.2 jmespath-1.0.1 kubernetes-30.1.0 langchain-0.1.20 langchain-cohere-0.1.5 langchain-community-0.0.38 langchain-core-0.1.52 langchain-openai-0.1.7 langchain-text-splitters-0.0.2 marshmallow-3.21.3 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.18.0 openai-1.33.0 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 overrides-7.7.0 parameterized-0.9.0 posthog-3.5.0 pypdf-4.2.0 pypika-0.48.9 pyreqwest-impersonate-0.4.7 pysbd-0.3.4 python-dotenv-1.0.1 regex-2023.12.25 s3transfer-0.10.1 schema-0.7.7 starlette-0.37.2 tiktoken-0.7.0 tokenizers-0.15.2 types-requests-2.32.0.20240602 typing-inspect-0.9.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install duckduckgo-search crewai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no-EhZT58YU1"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "Groq_llm = ChatGroq(api_key = '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', model = 'gemma-7b-it')\n",
        "from crewai import Crew, Agent, Task, Process\n",
        "from langchain_community.tools import DuckDuckGoSearchRun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lBz6NNh-sqd"
      },
      "outputs": [],
      "source": [
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiMPvwju8gSs"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "search_tool = DuckDuckGoSearchRun()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvGVipvo8kwl"
      },
      "outputs": [],
      "source": [
        "import json  # Import the JSON module to parse JSON strings\n",
        "from langchain_core.agents import AgentFinish\n",
        "\n",
        "agent_finishes  = []\n",
        "\n",
        "import json\n",
        "from typing import Union, List, Tuple, Dict\n",
        "from langchain.schema import AgentFinish\n",
        "\n",
        "call_number = 0\n",
        "\n",
        "def print_agent_output(agent_output: Union[str, List[Tuple[Dict, str]], AgentFinish], agent_name: str = 'Generic call'):\n",
        "    global call_number  # Declare call_number as a global variable\n",
        "    call_number += 1\n",
        "    with open(\"crew_callback_logs.txt\", \"a\") as log_file:\n",
        "        if isinstance(agent_output, str):\n",
        "            try:\n",
        "                agent_output = json.loads(agent_output)\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "\n",
        "        if isinstance(agent_output, list) and all(isinstance(item, tuple) for item in agent_output):\n",
        "            print(f\"-{call_number}----Dict------------------------------------------\", file=log_file)\n",
        "            for action, description in agent_output:\n",
        "                # Print attributes based on assumed structure\n",
        "                print(f\"Agent Name: {agent_name}\", file=log_file)\n",
        "                print(f\"Tool used: {getattr(action, 'tool', 'Unknown')}\", file=log_file)\n",
        "                print(f\"Tool input: {getattr(action, 'tool_input', 'Unknown')}\", file=log_file)\n",
        "                print(f\"Action log: {getattr(action, 'log', 'Unknown')}\", file=log_file)\n",
        "                print(f\"Description: {description}\", file=log_file)\n",
        "                print(\"--------------------------------------------------\", file=log_file)\n",
        "\n",
        "        # Check if the output is a dictionary as in the second case\n",
        "        elif isinstance(agent_output, AgentFinish):\n",
        "            print(f\"-{call_number}----AgentFinish---------------------------------------\", file=log_file)\n",
        "            print(f\"Agent Name: {agent_name}\", file=log_file)\n",
        "            agent_finishes.append(agent_output)\n",
        "            # Extracting 'output' and 'log' from the nested 'return_values' if they exist\n",
        "            output = agent_output.return_values\n",
        "            # log = agent_output.get('log', 'No log available')\n",
        "            print(f\"AgentFinish Output: {output['output']}\", file=log_file)\n",
        "            # print(f\"Log: {log}\", file=log_file)\n",
        "            # print(f\"AgentFinish: {agent_output}\", file=log_file)\n",
        "            print(\"--------------------------------------------------\", file=log_file)\n",
        "\n",
        "        # Handle unexpected formats\n",
        "        else:\n",
        "            # If the format is unknown, print out the input directly\n",
        "            print(f\"-{call_number}-Unknown format of agent_output:\", file=log_file)\n",
        "            print(type(agent_output), file=log_file)\n",
        "            print(agent_output, file=log_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f66hJEI-8_Zi"
      },
      "outputs": [],
      "source": [
        "class material_generator():\n",
        "    def entity_recognizer(self):\n",
        "        return Agent(\n",
        "                role = 'Sentence constructor',\n",
        "                goal = '''For the given entities construct meaningful sentences''',\n",
        "                backstory='''You are a master at framing sentences''',\n",
        "                llm = Groq_llm,\n",
        "                verbose=False,\n",
        "                allow_delegation=False,\n",
        "                max_iter=5,\n",
        "                memory=True,\n",
        "                step_callback=lambda x: print_agent_output(x,'Named entity recognition'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COiKKk2i9OC3"
      },
      "outputs": [],
      "source": [
        "class task_manager():\n",
        "    # Define your tasks with descriptions and expected outputs\n",
        "    def topic_reco(self, data):\n",
        "        return Task(\n",
        "            description=\"\"\"Conduct a comprehensive analysis and grasp the general topic and identify your learning goals\"\"\",\n",
        "            expected_output= 'a list of topics and learning goals',\n",
        "            output_file=f\"topics.txt\",\n",
        "            agent=ner_agent\n",
        "            )\n",
        "\n",
        "    def material_maker(self, data):\n",
        "        return Task(\n",
        "            description='''Analyze the topics given and make a complete notes accordingly.\n",
        "                focus on key ideas don't try to write everything down word-for-word.\n",
        "                Instead, focus on captu+ring the main points, definitions, arguments, and conclusions.\n",
        "                Be selective: Not all details are essential. Use bullet points, abbreviations, and symbols\n",
        "                to condense information efficiently.Use headings, subheadings, numbering, and indentation to create a clear hierarchy of information.Pleasen do not repeat sentences''',\n",
        "            expected_output=\"\"\"A large chuck of prepared notes\"\"\",\n",
        "            context = [topic_model],\n",
        "            output_file=f\"output_list.txt\",\n",
        "            agent=data_gen\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u_ZeRcr9mdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generator(data):\n",
        "      agents = material_generator()\n",
        "      tasks = task_manager()\n",
        "  #agents\n",
        "      ner_agent = agents.entity_recognizer()\n",
        "      data_gen = agents.notes_maker()\n",
        "\n",
        "  #tasks\n",
        "      topic_model = tasks.topic_reco(data)\n",
        "      notes = tasks.material_maker(data)\n",
        "      crew = Crew(\n",
        "        agents=[ner_agent,data_gen],\n",
        "        tasks=[topic_model, notes],\n",
        "        verbose = 2,\n",
        "        process = Process.sequential,\n",
        "        full_output=True,\n",
        "        share_crew = False,\n",
        "        step_callback = lambda x: print_agent_output(x,\"MasterCrew Agent\"))\n",
        "      results = crew.kickoff()\n",
        "      d1 = list(results.values())\n",
        "      tasks = d1[1]\n",
        "      full_dictionary = []\n",
        "      for i in tasks:\n",
        "          newdict = dict(i)\n",
        "          full_dictionary.append(newdict)\n",
        "      return full_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0z7bUHp_cjN"
      },
      "outputs": [],
      "source": [
        "pip install reportLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkb8im3G-jNs"
      },
      "outputs": [],
      "source": [
        "notes = generator(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmak3Pw0CfYQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfTS51KjCRDI"
      },
      "outputs": [],
      "source": [
        "https://153d-34-91-237-152.ngrok-free.app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU2WezJQ32dG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS05R4gvZNxd"
      },
      "outputs": [],
      "source": [
        "print(public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W52wfYgHXFNO"
      },
      "outputs": [],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHmX6wv4xTAY"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "!ngrok config add-authtoken 2fh3RRrTOnb4LIpFOyDh31DajTd_7T3TkvLzcjBaa9VBGhXw4\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Print URL:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wml3e9yUjxGX"
      },
      "outputs": [],
      "source": [
        "pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDhP9RW5OJVz"
      },
      "outputs": [],
      "source": [
        "import face_recognition as fr\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRZyhxXJ17GR"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import os\n",
        "\n",
        "class VideoPreprocessor:\n",
        "    def __init__(self, directory, video_path):\n",
        "        self.directory = directory\n",
        "        self.video_path = video_path\n",
        "        self.fps = None\n",
        "        self.frame_count = 0\n",
        "        self.extracted_frame_count = 0\n",
        "\n",
        "    def empty_folder(self):\n",
        "        for file in os.listdir(self.directory):\n",
        "            file_path = os.path.join(self.directory, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        return True\n",
        "\n",
        "    def extract_frames_per_second(self):\n",
        "        try:\n",
        "            cap = cv.VideoCapture(self.video_path)\n",
        "            if not cap.isOpened():\n",
        "                raise IOError(\"Error opening video!\")\n",
        "\n",
        "            self.fps = cap.get(cv.CAP_PROP_FPS)\n",
        "            if self.fps <= 0:\n",
        "                raise ValueError(\"Invalid FPS value.\")\n",
        "\n",
        "            if self.empty_folder():\n",
        "                while True:\n",
        "                    ret, frame = cap.read()\n",
        "\n",
        "                    if not ret:\n",
        "                        print(\"End of video reached.\")\n",
        "                        break\n",
        "\n",
        "                    # Extract one frame per second\n",
        "                    if self.frame_count % int(self.fps) == 0:\n",
        "                        filename = f\"{self.directory}/frame_{self.extracted_frame_count}.jpg\"\n",
        "                        cv.imwrite(filename, frame)\n",
        "                        self.extracted_frame_count += 1\n",
        "\n",
        "                    self.frame_count += 1\n",
        "\n",
        "                print(f\"Extracted {self.extracted_frame_count} frames to {self.directory}\")\n",
        "\n",
        "        except IOError as e:\n",
        "            print(f\"IOError: {e}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"ValueError: {e}\")\n",
        "        finally:\n",
        "            cap.release()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwvo6NqNYoUu"
      },
      "source": [
        "CLASSES TEST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip5myxmsYmrt"
      },
      "outputs": [],
      "source": [
        "import face_recognition as fr\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class FaceRecognition:\n",
        "    def __init__(self, frames_dir, known_face_encodings, student_dic):\n",
        "        self.frames_dir = frames_dir\n",
        "        self.known_face_encodings = known_face_encodings\n",
        "        self.students = student_dic\n",
        "\n",
        "        csm_students = list(self.students.keys())\n",
        "\n",
        "    def load_image_paths(self):\n",
        "        image_paths = [os.path.join(self.frames_dir, filename) for filename in os.listdir(self.frames_dir) if filename.lower().endswith('.jpg')]\n",
        "        return image_paths\n",
        "\n",
        "    def find_faces(self, image_paths):\n",
        "        face_locations = []\n",
        "        face_encodings = []\n",
        "\n",
        "        for path in image_paths:\n",
        "            frame = fr.load_image_file(path)\n",
        "            rgb_frame = frame[:, :, ::-1]  # Convert BGR to RGB\n",
        "\n",
        "            face_locations = fr.face_locations(rgb_frame, number_of_times_to_upsample=1, model='hog')\n",
        "            face_encodings_for_frame = fr.face_encodings(frame, face_locations)\n",
        "\n",
        "            face_encodings.extend(face_encodings_for_frame)\n",
        "\n",
        "        return face_encodings\n",
        "\n",
        "\n",
        "\n",
        "    def cosine_similarity(self, vector_a, vector_b):\n",
        "        vector_a = np.array(vector_a, dtype=float)\n",
        "        vector_b = np.array(vector_b, dtype=float)\n",
        "        dot_product = np.dot(vector_a, vector_b)\n",
        "        magnitude_a = np.linalg.norm(vector_a)\n",
        "        magnitude_b = np.linalg.norm(vector_b)\n",
        "\n",
        "        if magnitude_a == 0 or magnitude_b == 0:\n",
        "            return 0\n",
        "\n",
        "        cosine_similarity_value = dot_product / (magnitude_a * magnitude_b)\n",
        "        return cosine_similarity_value\n",
        "\n",
        "    def compare_faces(self, video_face_encodings):\n",
        "        similarities = []\n",
        "        student_names = []\n",
        "        csm_students = list(self.students.keys())\n",
        "\n",
        "        for y in video_face_encodings:\n",
        "            nec = [self.cosine_similarity(y, x) for x in self.known_face_encodings]\n",
        "            similarities.append(nec)\n",
        "\n",
        "        for x in similarities:\n",
        "            student_names.append(csm_students[np.argmax(x)])\n",
        "\n",
        "        return list(set(student_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdIH_8kfmDsg"
      },
      "outputs": [],
      "source": [
        "from Face_recog_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbXE2VJvdn71"
      },
      "outputs": [],
      "source": [
        "from flask import *\n",
        "from VideoPreoprocess import VideoPreprocessor\n",
        "from Face_recog_methods_2 import FaceRecognition\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import os\n",
        "app = Flask(__name__)\n",
        "app.secret_key=\"I_SAVE_GOTHAM\"\n",
        "@app.route('/')\n",
        "def videoUploadFile():\n",
        "    return render_template('index.html')\n",
        "\n",
        "ALLOWED_EXTENSIONS = ['mp4']\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "@app.route(\"/upload\",methods = ['POST',\"GET\"])\n",
        "def uploadStudents():\n",
        "    if 'video' not in request.files:\n",
        "        return  jsonify(\"No Video Found\")\n",
        "    video = request.files['video']\n",
        "    if video.filename == \"\":\n",
        "        return jsonify(\"No File Selected\")\n",
        "    if video and allowed_file(video.filename):\n",
        "        video.save(video.filename)\n",
        "        video_object = VideoPreprocessor(\"Face-Attendance-Server-main/Frames\",video_path=video.filename)\n",
        "        if video_object.empty_folder():\n",
        "            video_object.extract_frames_per_second()\n",
        "            studentdetails,known_encodings = fetchFromDataBase()\n",
        "            face_object = FaceRecognition(\"Face-Attendance-Server-main/Frames\",known_encodings,studentdetails)\n",
        "            new_image_paths = face_object.load_image_paths()\n",
        "            new_face_encodings = face_object.find_faces(new_image_paths)\n",
        "            attendees_list = face_object.compare_faces(new_face_encodings)\n",
        "            print(attendees_list)\n",
        "            return jsonify(attendees_list)\n",
        "        return jsonify(\"Error in emptying the Folder\")\n",
        "    return jsonify(\"Invalid type in the File \")\n",
        "def fetchFromDataBase():\n",
        "    with sqlite3.connect(\"Attendance.db\") as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT * FROM USERS\")\n",
        "        student_dict = {}\n",
        "        new_Face_Encodings = []\n",
        "        for i in cursor :\n",
        "            student_dict[i[1]] = i[0]\n",
        "            new_Face_Encodings.append([float(j) for j in i[2].split(\"/\") ])\n",
        "    return student_dict,new_Face_Encodings\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGbw6PFidiuH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wH5Hjh4bMBc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}